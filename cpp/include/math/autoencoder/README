| **Main Type**                       | **Sub-variant Name(s)**                                         | **Core Idea**                                           | **Loss / Regularization**                   | **Applications**                     |
| ----------------------------------- | --------------------------------------------------------------- | ------------------------------------------------------- | ------------------------------------------- | ------------------------------------ |
| **1. Basic Autoencoder**            | -                                                               | Learn to reconstruct input                              | MSE, BCE                                    | Dimensionality reduction             |
| **2. Undercomplete AE**             | -                                                               | Latent dim < input dim                                  | MSE                                         | Compression, denoising               |
| **3. Overcomplete AE**              | -                                                               | Latent dim > input dim                                  | L1, L2 regularization                       | Feature extraction                   |
| **4. Denoising AE**                 | -                                                               | Corrupt input → recover original                        | MSE, Gaussian/Bernoulli noise               | Robust encoding                      |
| **5. Sparse AE**                    | -                                                               | Enforce sparsity on latent units                        | L1 norm, KL divergence                      | Part-based features                  |
| **6. Contractive AE**               | -                                                               | Penalize sensitivity of encoding to input               | \$\lambda \|\nabla\_x f(x)\|^2\$            | Robust manifold learning             |
| **7. Variational AE (VAE)**         | β-VAE, CVAE, VQ-VAE, InfoVAE, FactorVAE, DIP-VAE, VampPrior-VAE | Probabilistic latent space; KL-regularized              | ELBO loss                                   | Generative modeling, disentanglement |
| **8. Adversarial AE (AAE)**         | Semi-supervised AAE, Domain AAE                                 | Match latent posterior with prior via discriminator     | Reconstruction + adversarial loss           | Domain adaptation, generative        |
| **9. Wasserstein AE (WAE)**         | WAE-MMD, WAE-GAN                                                | Wasserstein distance instead of KL in latent            | \$\mathcal{L}\_{rec} + \lambda D\_Z\$       | Stable generative models             |
| **10. Robust AE**                   | Robust DAE, L21-AE, Huber AE                                    | Handle noisy or outlier data                            | L1, L21 norm, Huber loss                    | Anomaly detection                    |
| **11. Convolutional AE (CAE)**      | -                                                               | CNN-based encoder/decoder                               | MSE                                         | Image compression, vision tasks      |
| **12. Sequence AE**                 | RNN-AE, LSTM-AE, GRU-AE, BiLSTM-AE                              | Encode and decode sequential data                       | Seq MSE, teacher forcing                    | Time-series, NLP                     |
| **13. Attention-based AE**          | Transformer-AE, MAE (Masked AE), Perceiver-AE                   | Use self-attention to model input                       | Masked MSE or BCE                           | NLP, Vision, Speech                  |
| **14. Structured AE**               | Graph AE, GCN-AE, Tree AE, Set AE                               | Works on structured inputs like graphs or trees         | Graph recon loss, node-wise loss            | Graph learning, structure modeling   |
| **15. Energy-based AE**             | Energy AE, EBM-AE                                               | Latent code assigned energy; contrastive regularization | Energy loss, contrastive divergence         | Out-of-distribution detection        |
| **16. Latent-space Regularized AE** | Latent Consistency AE, Triplet AE, Center AE                    | Regularize relationships in latent space                | Latent MSE, Triplet loss                    | Discriminative embeddings            |
| **17. Self-supervised AE**          | MAE, SimMIM, BYOL-AE, DINO-AE                                   | Predict missing parts / self-predict                    | Masked loss, contrastive loss               | Representation learning              |
| **18. Hybrid AE Models**            | VAE-GAN, AAE-VAE, AE-GAN, VAE-AAE                               | Combine strengths of multiple AE types                  | Hybrid loss (ELBO + GAN)                    | Generative, synthesis                |
| **19. Disentangled AE**             | TC-VAE, FactorVAE, Ada-GVAE, StyleVAE                           | Latent variables encode distinct generative factors     | Total correlation regularization            | Causal and interpretable models      |
| **20. Koopman AE / Dynamics AE**    | LinearDynamicalAE, LatentODE-AE, Neural ODE-AE                  | Impose linear or known dynamics in latent space         | Latent prediction error                     | Dynamical systems, control           |
| **21. Metric AE**                   | Siamese AE, Triplet AE, Contrastive AE                          | Supervised AE using pairwise or triplet relationships   | Contrastive / Triplet loss + Recon          | Face ID, retrieval                   |
| **22. Bayesian AE**                 | Bayes-AE, Probabilistic AE, Stochastic-AE                       | Bayesian treatment of encoder/decoder weights           | VI loss, ELBO                               | Uncertainty estimation               |
| **23. Physics-Informed AE**         | PI-AE, PDE-AE, PINN-AE                                          | Encode physics via constraints in loss                  | PDE residual loss + MSE                     | Scientific ML                        |
| **24. Federated AE**                | FedAE, SplitAE, Differentially Private AE                       | Train across clients; preserve data privacy             | FedAvg, noise injection                     | Federated, edge ML                   |
| **25. Diffusion AE**                | Diff-AE, D3PM-AE, Denoise-AE                                    | Train AE as part of diffusion process                   | Denoising score matching, variational bound | Generation, denoising                |
| **26. Quantized AE**                | VQ-AE, VQ-VAE, Discrete AE                                      | Latent is discrete via quantization                     | Commitment loss + Recon loss                | Speech, compression                  |
| **27. Equivariant AE**              | G-AE, SE(3)-AE, E(2)-AE                                         | Preserve group symmetries (rotation, etc.)              | Group-equivariant loss                      | Molecular modeling, physics          |
| **28. Cross-modal AE**              | Audio-Visual AE, Text-Image AE, Multimodal AE                   | Encode multiple modalities to shared latent             | Multi-modal recon + alignment loss          | Cross-modal retrieval                |
| **29. Swapped / Mixup AE**          | Mixup-AE, Swapped-AE, Latent Interpolation AE                   | Swap or interpolate latent codes                        | Mixup loss + AE loss                        | Regularization, generalization       |
| **30. Contrastive AE**              | SimCLR-AE, MoCo-AE, CL-AE                                       | Contrastive loss in latent + reconstruction             | Contrastive + MSE                           | Self-supervised learning             |
