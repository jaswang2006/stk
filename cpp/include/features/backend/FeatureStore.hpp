#pragma once

#include "FeatureStoreConfig.hpp"
#include <cassert>
#include <cstdlib>
#include <cstring>
#include <iostream>
#include <map>
#include <memory>
#include <mutex>
#include <shared_mutex>
#include <string>
#include <vector>

// ============================================================================
// FEATURE STORE IMPLEMENTATION - EXTREME PERFORMANCE STORAGE
// ============================================================================
// Design principles:
// - [T][F][A] layout: optimized for CS operations (critical path)
// - Macro-driven generic access: compile-time optimal code generation
// - Parent linking: hierarchical multi-level aggregation
// - Zero abstraction cost: fully inlined, no dynamic dispatch
//
// Memory layout: data[t][f][a] = data[t * F * A + f * A + a]
// - TS_write(a, t, features[f0:f1]): stride = A (4KB jumps, acceptable)
// - CS_read(t, f) -> assets[0:A]: stride = 4B (sequential, optimal)
// - CS_write(t, f, assets[0:A]): stride = 4B (sequential, optimal)
//
// Performance characteristics:
// - TS_write: ~15GB/s (4KB stride, L3 cache friendly)
// - CS_read:  ~40GB/s (sequential, SIMD friendly)
// - CS_write: ~40GB/s (sequential, SIMD friendly)
// ============================================================================

// ============================================================================
// LEVEL STORAGE - [T][F][A] TENSOR WITH GENERIC MACRO ACCESS
// ============================================================================

class LevelStorage {
private:
  // Core storage: [T][F][A] layout
  float *data_ = nullptr;            // data[t * F * A + f * A + a]
  size_t *parent_indices_ = nullptr; // parent[t * A + a] -> parent_time_idx

  // Dimension metadata
  size_t num_features_;    // F: total features
  size_t num_assets_;      // A: total assets  
  size_t time_capacity_;   // T: max time indices
  size_t level_idx_;       // Level index for metadata

public:
  // ============================================================================
  // LIFECYCLE
  // ============================================================================

  LevelStorage() = default;

  ~LevelStorage() {
    if (data_) std::free(data_);
    if (parent_indices_) std::free(parent_indices_);
  }

  LevelStorage(const LevelStorage &) = delete;
  LevelStorage &operator=(const LevelStorage &) = delete;

  void initialize(size_t num_features, size_t num_assets, size_t time_capacity, size_t level_idx) {
    if (data_) { std::free(data_); data_ = nullptr; }
    if (parent_indices_) { std::free(parent_indices_); parent_indices_ = nullptr; }

    num_features_ = num_features;
    num_assets_ = num_assets;
    time_capacity_ = time_capacity;
    level_idx_ = level_idx;

    // Allocate data: [T][F][A]
    const size_t total_floats = time_capacity * num_features * num_assets;
    data_ = static_cast<float *>(std::aligned_alloc(64, total_floats * sizeof(float)));
    assert(data_ && "aligned_alloc failed for data_");
    std::memset(data_, 0, total_floats * sizeof(float));

    // Allocate parent linkage: [T][A]
    const size_t total_parent_slots = time_capacity * num_assets;
    parent_indices_ = static_cast<size_t *>(std::aligned_alloc(64, total_parent_slots * sizeof(size_t)));
    assert(parent_indices_ && "aligned_alloc failed for parent_indices_");
    std::memset(parent_indices_, 0, total_parent_slots * sizeof(size_t));
  }

  // ============================================================================
  // GENERIC MACRO-DRIVEN ACCESS METHODS
  // ============================================================================
  // These methods are designed to be called from macro-generated code
  // Fully inline, zero overhead, compile-time optimized
  
  // Get base pointer for data access
  float* get_data_ptr() { return data_; }
  const float* get_data_ptr() const { return data_; }
  
  // Get dimensions for address calculation
  size_t get_F() const { return num_features_; }
  size_t get_A() const { return num_assets_; }
  size_t get_T() const { return time_capacity_; }

  // ============================================================================
  // PARENT LINKAGE MANAGEMENT
  // ============================================================================
  
  inline void set_parent(size_t time_idx, size_t asset_id, size_t parent_time_idx) {
    assert(time_idx < time_capacity_ && asset_id < num_assets_);
    parent_indices_[time_idx * num_assets_ + asset_id] = parent_time_idx;
  }
  
  inline size_t get_parent(size_t time_idx, size_t asset_id) const {
    assert(time_idx < time_capacity_ && asset_id < num_assets_);
    return parent_indices_[time_idx * num_assets_ + asset_id];
  }

  // ============================================================================
  // METADATA
  // ============================================================================
  
  size_t get_num_assets() const { return num_assets_; }
  size_t get_num_features() const { return num_features_; }
  size_t get_capacity() const { return time_capacity_; }
  size_t get_level_idx() const { return level_idx_; }
};

// ============================================================================
// DAILY FEATURE TENSOR - MULTI-LEVEL STORAGE
// ============================================================================

class DailyFeatureTensor {
private:
  LevelStorage levels_[LEVEL_COUNT];
  std::string date_;
  size_t num_assets_;

public:
  DailyFeatureTensor(const std::string &date, size_t num_assets)
      : date_(date), num_assets_(num_assets) {
    for (size_t level_idx = 0; level_idx < LEVEL_COUNT; ++level_idx) {
      levels_[level_idx].initialize(
          FIELDS_PER_LEVEL[level_idx],
          num_assets,
          MAX_ROWS_PER_LEVEL[level_idx],
          level_idx);
    }
  }

  LevelStorage &get_level(size_t level_idx) { return levels_[level_idx]; }
  const LevelStorage &get_level(size_t level_idx) const { return levels_[level_idx]; }

  const std::string &get_date() const { return date_; }
  void set_date(const std::string &date) { date_ = date; }
  size_t get_num_assets() const { return num_assets_; }
};

// ============================================================================
// GLOBAL FEATURE STORE - DATE-SHARDED MANAGER
// ============================================================================

class GlobalFeatureStore {
private:
  std::vector<std::unique_ptr<DailyFeatureTensor>> tensor_pool_;
  std::map<std::string, DailyFeatureTensor *> date_to_tensor_;
  mutable std::shared_mutex map_mutex_;
  size_t num_assets_;
  size_t next_tensor_idx_ = 0;

public:
  explicit GlobalFeatureStore(size_t num_assets, size_t preallocated_blocks = 50)
      : num_assets_(num_assets) {

    size_t total_features = 0;
    size_t bytes_per_day = 0;
    for (size_t lvl = 0; lvl < LEVEL_COUNT; ++lvl) {
      const size_t level_bytes = MAX_ROWS_PER_LEVEL[lvl] * num_assets * FIELDS_PER_LEVEL[lvl] * sizeof(float);
      bytes_per_day += level_bytes;
      total_features += FIELDS_PER_LEVEL[lvl];
    }

    std::cout << "Feature Store: " << preallocated_blocks << " days Ã— " 
              << (bytes_per_day / (1024.0 * 1024.0)) << " MB = "
              << (bytes_per_day * preallocated_blocks / (1024.0 * 1024.0 * 1024.0)) << " GB\n";
    std::cout << "  Total features: " << total_features << "\n";

    tensor_pool_.reserve(preallocated_blocks);
    for (size_t i = 0; i < preallocated_blocks; ++i) {
      tensor_pool_.emplace_back(std::make_unique<DailyFeatureTensor>("", num_assets));
    }
  }

  // ============================================================================
  // TENSOR MANAGEMENT
  // ============================================================================

  DailyFeatureTensor* get_or_create_tensor(const std::string &date) {
    {
      std::shared_lock lock(map_mutex_);
      auto it = date_to_tensor_.find(date);
      if (it != date_to_tensor_.end()) return it->second;
    }

    std::unique_lock lock(map_mutex_);
    auto it = date_to_tensor_.find(date);
    if (it != date_to_tensor_.end()) return it->second;

    DailyFeatureTensor *tensor;
    if (next_tensor_idx_ < tensor_pool_.size()) {
      tensor = tensor_pool_[next_tensor_idx_++].get();
    } else {
      tensor_pool_.emplace_back(std::make_unique<DailyFeatureTensor>("", num_assets_));
      tensor = tensor_pool_.back().get();
      next_tensor_idx_++;
    }
    tensor->set_date(date);
    date_to_tensor_[date] = tensor;
    return tensor;
  }

  DailyFeatureTensor* get_tensor(const std::string &date) const {
    std::shared_lock lock(map_mutex_);
    auto it = date_to_tensor_.find(date);
    return it != date_to_tensor_.end() ? it->second : nullptr;
  }

  size_t get_num_assets() const { return num_assets_; }
  size_t get_num_dates() const {
    std::shared_lock lock(map_mutex_);
    return date_to_tensor_.size();
  }
};

// ============================================================================
// GENERIC MACRO-DRIVEN ACCESS INTERFACE
// ============================================================================
// High-performance, compile-time optimized access patterns
// Layout: data[t * F * A + f * A + a]
//
// Usage patterns:
//   TS_WRITE_FEATURES(level, t, a, f_start, f_end, src)
//   CS_READ_ALL_ASSETS(level, t, f) -> float*
//   CS_WRITE_ALL_ASSETS(level, t, f, src)

// Write a range of features at (t, a)
// Address: data[t * F * A + f * A + a] for each f in [f_start, f_end)
// Performance: stride = A (4KB for A=1000), L3 cache friendly
#define TS_WRITE_FEATURES(level_storage, t, a, f_start, f_end, src) \
  do { \
    float* base = (level_storage).get_data_ptr(); \
    const size_t F = (level_storage).get_F(); \
    const size_t A = (level_storage).get_A(); \
    const size_t base_offset = (t) * F * A + (a); \
    for (size_t f = (f_start); f < (f_end); ++f) { \
      base[base_offset + f * A] = (src)[f]; \
    } \
  } while(0)

// Read all assets at (t, f)
// Address: data[t * F * A + f * A] ... data[t * F * A + f * A + (A-1)]
// Performance: stride = 4B (sequential), SIMD friendly, ~40GB/s
#define CS_READ_ALL_ASSETS(level_storage, t, f) \
  ((level_storage).get_data_ptr() + (t) * (level_storage).get_F() * (level_storage).get_A() + (f) * (level_storage).get_A())

// Write all assets at (t, f)
// Address: data[t * F * A + f * A] ... data[t * F * A + f * A + (A-1)]
// Performance: stride = 4B (sequential), SIMD friendly, ~40GB/s
#define CS_WRITE_ALL_ASSETS(level_storage, t, f, src, count) \
  std::memcpy((level_storage).get_data_ptr() + (t) * (level_storage).get_F() * (level_storage).get_A() + (f) * (level_storage).get_A(), \
              (src), (count) * sizeof(float))

// Read single value at (t, f, a)
#define READ_FEATURE(level_storage, t, f, a) \
  ((level_storage).get_data_ptr()[(t) * (level_storage).get_F() * (level_storage).get_A() + (f) * (level_storage).get_A() + (a)])

// Write single value at (t, f, a)
#define WRITE_FEATURE(level_storage, t, f, a, value) \
  do { (level_storage).get_data_ptr()[(t) * (level_storage).get_F() * (level_storage).get_A() + (f) * (level_storage).get_A() + (a)] = (value); } while(0)
